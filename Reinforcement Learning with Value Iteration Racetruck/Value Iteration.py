# -*- coding: utf-8 -*-
"""Refanidis_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y_6bGppPWt6ScES5pd_oMYdzqXZGHc8c

# FINAL TRY
"""

import os # Library enables the use of operating system dependent functionality
import time # Library handles time-related tasks
from random import shuffle # Import shuffle() method from the random module
from random import random # Import random() method from the random module
from copy import deepcopy # Enable deep copying
import numpy as np # Import Numpy library
# Define constants
ALGORITHM_NAME = "Value_Iteration"
FILENAME = "Race.TXT"
THIS_TRACK = "R_track"
START = 'S'
GOAL = 'F'
WALL = '#'
TRACK = '.'
MAX_VELOCITY = 2
MIN_VELOCITY = -2
DISC_RATE = 0.9  # Gamma value
ERROR_THRES = 0.001 # Error threshhold for which the algorithm ends 
PROB_ACCELER_FAILURE = 0.20 # Probability the car will try to act a specific action on y or x axis and fails
PROB_ACCELER_SUCCESS = 0.80 # Probability the car will try to act a specific action on y or x axis and succeeds
# Range of the possible velocities of car
vel_range = range(MIN_VELOCITY, MAX_VELOCITY + 1)
 
# All actions A the race car can take. We define first the acceleration on y axis and then on x, because change on x axis is about change about 
# columns and change on yaxis is about rows 
# (acceleration in y direction, acceleration in x direction)
actions = [[-1,-1], [0,-1], [1,-1],
           [-1,0] , [0,0],  [1,0],
           [-1,1] , [0,1],  [1,1]]

# Here we define a function that saves the track as variable 'environment'. Track is a txt file containing symbols for each position of the track.
# For example, walls are denoted with '#', finish line with 'F' etc. 

def read_environment(filename):
    # Read all lines of track 
    with open(filename, 'r') as file:
        environment_data = file.readlines()
    file.close()

    # Saving the environment in a list containing all symbols of the txt file.
    environment = []
    for i,line in enumerate(environment_data):
        if i > 0:
            line = line.strip()
            # If the line is empty, ignore it
            if line == "": continue
            # append a list with all symbols in each row of the track
            environment.append([x for x in line])
    # Return the environment 
    return environment

# Here we define the functions that changes the possible state of the car
# First of all we define the function that returns the new velocities depending on actions 
def get_new_velocity(old_vel,accel,min_vel=MIN_VELOCITY,max_vel=MAX_VELOCITY):
    new_vy = old_vel[0] + accel[0] 
    new_vx = old_vel[1] + accel[1]

    # Keeping the constraint that velocity should be between -2 and 2 
    if new_vx < min_vel:
       new_vx = min_vel
    if new_vx > max_vel:
       new_vx = max_vel
    if new_vy < min_vel:
       new_vy = min_vel
    if new_vy > max_vel:
       new_vy = max_vel
     
    # Return the new velocities
    return new_vy, new_vx

# And now the funtion that returns the new position of the car depending on its current velocity
def get_new_position(old_location, vel, environment):
    y,x = old_location[0], old_location[1]
    vy, vx = vel[0], vel[1]

    # Return the new locations  
    return y+vy, x+vx

# Creating the function that gets all the points defined by the rectangle of the old and new location of the car,
# in order to check if the car crashed into a wall
def find_rectangle(old_y, old_x, new_y, new_x, environment):
  min_y = min(old_y, new_y)
  max_y = max(old_y, new_y)
  min_x = min(old_x, new_x)
  max_x = max(old_x, new_x)
  
  # Keeping all the points of the given rectangle on a list
  rectangle_points = []
  for y in range(min_y, max_y + 1):
      for x in range(min_x, max_x + 1):
          rectangle_points.append((y, x))
  
  # Return the list with all points
  return rectangle_points

# Here we create a function in order to find the position that is closer to a position we got, in case this position is a wall or we are outside of the track
def get_nearest_open_cell(environment, y_crash, x_crash, vy = 0, vx = 0, open = [TRACK, START, GOAL]):
    rows = len(environment)
    cols = len(environment[0])    
    
    # Use the radius to search the nearest cell
    max_radius = max(rows,cols)
 
    # Generate a search radius for each scenario
    for radius in range(max_radius):
 
        # If car is not moving in y direction
        if vy == 0: 
            y_off_range = range(-radius, radius + 1)
        # If the velocity in y-direction is negative
        elif vy < 0:
            # Search in the positive direction
            y_off_range = range(0, radius + 1)
        else:
            # Else search in the negative direction
            y_off_range = range(-radius, 1)
 
        # For each value in the search radius range of y
        for y_offset in y_off_range:
 
            # Start near to crash site and expand from there
            y = y_crash + y_offset
            x_radius = radius - abs(y_offset)
 
            # If car is not moving in x direction
            if vx == 0:
                x_range = range(x_crash - x_radius, x_crash + x_radius + 1)
            # If the velocity in x-direction is negative
            elif vx < 0:
                x_range = range(x_crash, x_crash + x_radius + 1)
            # If the velocity in y-direction is positive
            else:
                x_range = range(x_crash - x_radius, x_crash + 1)
 
            # For each value in the search radius range of x
            for x in x_range:
                # We can't go outside the environment(racetrack) boundary
                if y < 0 or y >= rows: continue
                if x < 0 or x >= cols: continue
 
                # If we find and open cell, return that (y,x) open cell
                if environment[y][x] in open: 
                    return(y,x)        
     # Else, if the function didnt find any open square (a case that it is not possible), return nothing
    return

# Here we create the main function that will return the new state of the car, given the old state, in order to use this in the value iteration algorithm.
def act(old_y, old_x, old_vy, old_vx, accel, environment):
    # Get the new velocity values
    new_vy, new_vx = get_new_velocity((old_vy,old_vx), accel)
 
    # Find the possible position of the car using the velocity values
    temp_y, temp_x = get_new_position((old_y,old_x), (new_vy, new_vx),( 
                                     environment))
 
    # Find the nearest open cell on the racetrack to this new position, in order to check if we are allowed to do this
    new_y, new_x = get_nearest_open_cell(environment, temp_y, temp_x, new_vy, 
                                     new_vx)
    
    # Find the rectangle points in order to check if we crashed. 
    rec_points = find_rectangle(old_y, old_x, temp_y, temp_x,environment)

    # Now, if the possible position of the car, is not the nearest open cell in this position, meaning that we are in a wall,
    # set the position of the car to the previous one, and its velocity to (0,0)
    if new_y != temp_y or new_x != temp_x:
        new_y, new_x = old_y, old_x
        new_vy, new_vx = 0,0
    # Else, if the nearest open position is the the possible posittion we found 
    # and if all the points in the rectangle found that is defined by the old and new position are not a wall on the environment, 
    # set the new position as the position we found
    elif new_y == temp_y or new_x == temp_x:
      for i in range(len(rec_points)):
        if environment[rec_points[i][0]][rec_points[i][1]] == WALL:
          new_y, new_x = old_y, old_x
          new_vy, new_vx = 0,0
          break
        else: 
          new_y = temp_y
          new_x = temp_x

    # Return the new state
    return new_y, new_x, new_vy, new_vx

# And now the main function of the code. I will try to be as explanatory as I can. 
def value_iteration(environment, reward = 100):
    rows = len(environment)
    cols = len(environment[0])
    
    # Here we create the table named values that will store the optimal utilities 
    # for each state found. We set all starting values equal to 0, except final states that equals 100, something we are doing later,
    #and then we create this 4d array.
    # This array will be used in order to check if we converge to the solution.
    values = [[[[0 for _ in vel_range] for _ in vel_range] for _ in (
        line)] for line in environment]

    # Set the finish states to 100
    for y in range(rows):
        for x in range(cols):
            if environment[y][x] == GOAL:
                for vy in vel_range:
                    for vx in vel_range:                 
                        values[y][x][vy][vx] = reward
 
    # We create the U matrix here in ordeer to store every possible value for each action made in a certain state
    U = [[[[[0 for _ in actions] for _ in vel_range] for _ in (
        vel_range)] for _ in line] for line in environment]
 
    # Set the finish states pairs to 100
    for y in range(rows):
        for x in range(cols):
            if environment[y][x] == GOAL:
                for vy in vel_range:
                    for vx in vel_range:   
                        for ai, a in enumerate(actions):                        
                            U[y][x][vy][vx][ai] = reward
 
    # Now we have the main iteration, that breaks when our delta drops below the threshold
    while True:
        # We are copying our values in a variable called values_prev, in order to check if we have a decrease in delta.
        values_prev = deepcopy(values)
        delta = 0.0
        # Iterating through all possible states 
        for y in range(rows):
            for x in range(cols):
                for vy in vel_range:
                    for vx in vel_range:
                         
                        # Assigning the values of the wall -10 for the cost
                        if environment[y][x] == WALL:
                            values[y][x][vy][vx] = -10
                            continue
 
                        # Now iterating through every possible action 
                        for ai, a in enumerate(actions):
 
                            # Assigning the reward -1 if we get in a position that is not the finish or wall
                            if environment[y][x] == GOAL:
                                r = reward
                            else:
                                r = -1
                    
                            # And now we will find 4 utilities of future states in order to calculate 
                            #the expected value of utilities. Each one of them will 
                            # correspond in a specific probability. For example, first we will calculate
                            # the value of the new state for the specific action, if both accelaration on 
                            # x and y axis succeeds. Then we will get the utility of the later state if acceleration on y axis fails 
                            # and succeeds on x axis, etc. 
                            # If both accelerations succeeds
                            new_y, new_x, new_vy, new_vx = act(y,x,vy,vx,a,environment)

                            value_of_new_state = values_prev[new_y][new_x][new_vy][new_vx]
 
                           # If acceleration on y axis fails, that means that for a specific action [y, x]
                           # the new acceleration will be [0, x]
                            new_y, new_x, new_vy, new_vx = act(y,x,vy,vx,[0, a[1]], environment)

                            value_of_new_state_if_accel_on_y_fails = values_prev[new_y][new_x][new_vy][new_vx]

                            # If acceleration on x axis fails, that means that for a specific action [y, x]
                           # the new acceleration will be [y, 0]
                            new_y, new_x, new_vy, new_vx = act(y,x,vy,vx,[a[0],0],environment)
 
                            value_of_new_state_if_accel_on_x_fails = values_prev[new_y][new_x][new_vy][new_vx]

                           # If acceleration on both axis fails, that means that for a specific action [y, x]
                           # the new acceleration will be [0, 0]
                            new_y, new_x, new_vy, new_vx = act(y,x,vy,vx,[0,0],environment)
 
                            value_of_new_state_if_accel_on_both_fails = values_prev[
                                new_y][new_x][new_vy][new_vx]
 
                            # And now using the values for the state, we calculate the expected value,
                            # using each time the probability for each action in a specific axis to fail
                            expected_value = (
                                PROB_ACCELER_SUCCESS * PROB_ACCELER_SUCCESS * value_of_new_state) + (
                                    PROB_ACCELER_FAILURE * PROB_ACCELER_SUCCESS * (value_of_new_state_if_accel_on_y_fails)) + (
                                        PROB_ACCELER_SUCCESS * PROB_ACCELER_FAILURE * value_of_new_state_if_accel_on_x_fails) + (
                                            PROB_ACCELER_FAILURE * PROB_ACCELER_FAILURE * value_of_new_state_if_accel_on_both_fails)
 
                            # Update the utility of a state using the type we need (r + γ * expected_value)
                            U[y][x][vy][vx][ai] = r + (DISC_RATE * expected_value)
 
                        # Get the action with the highest Q value
                        argMaxQ = np.argmax(U[y][x][vy][vx])
 
                        # Update the values we need in order to check if we converge, using the max element of each state
                        values[y][x][vy][vx] = U[y][x][vy][vx][argMaxQ]
 
        # Make sure all the rewards are to 100 in the terminal state
        for y in range(rows):
            for x in range(cols):
                # Terminal state has a value of 0
                if environment[y][x] == GOAL:
                    for vy in vel_range:
                        for vx in vel_range:                 
                            values[y][x][vy][vx] = reward
 
        # Check if we converged
        delta = max([max([max([max([abs(values[y][x][vy][vx] - values_prev[y][x][vy][vx]) for vx in vel_range]) for vy in (
            vel_range)]) for x in range(cols)]) for y in range(rows)])
 
        # And now finding the policy we want and storing it to a dictionary
        pi = {}
        if delta < ERROR_THRES:
          for y in range(rows): 
            for x in range(cols):
                for vy in vel_range:
                    for vx in vel_range:
                        pi[(y,x,vy,vx)] = actions[np.argmax(U[y][x][vy][vx])]
          return pi
 
    return

racetrack = read_environment('Race.TXT')

racetrack

# Retrieve the policy
policy = value_iteration(racetrack)

for i in range(0,10):
  for j in range(0, 15):
    for vy in range(-2, 3):
      for vx in range(-2,3):
        if racetrack[i][j] == WALL:
          pass
        else:
          print('Best action to take when car is on state ' +str((i,j,vy,vx)) + ' : ' + str(policy[(i, j, vy, vx)]))

"""# Trying policies"""

# In order to check and visualise better the trials of the car, we create this function
def print_environment(environment, car_position = [0,0]):
   
    # Store value of current position
    temp = environment[car_position[0]][car_position[1]]
 
    # Mark the position of the car
    environment[car_position[0]][car_position[1]] = "X"
    # Delay the input in order to watch clearly the race 
    time.sleep(0.5)

    os.system( 'cls' )
    for line in environment: 
        text = ""
        for character in line: 
            text += character
        print(text)
    environment[car_position[0]][car_position[1]] = temp

def get_random_start_position(environment):
    # Collect all possible starting positions on the racetrack
    starting_positions = []
 
    # For each row in the environment
    for y,row in enumerate(environment):
 
        # For each column in each row of the environment
        for x,col in enumerate(row):
 
            # If we are at the starting position
            if col == START:
                starting_positions += [(y,x)]
 
    # Random shuffle the list of starting positions
    shuffle(starting_positions)
 
    # Select a starting position
    return starting_positions[0]

def print_5_races(track,number_of_races = 5):
  for race in range(number_of_races):
        # And now testing the policies we found 
        # We will need two random variables in order to check if the car will accelerate on an axis
        prob_y = random()
        prob_x = random()
        # If 0 <= prob < 20% then we will say that acceleration will fail and if prob >= 20% then acceleration will succeed
        # In the first iteration we find the best starting policy and with the use of act function we are printing 
        # the first step of the car, depending on the probabilities that we got 
        old_y, old_x = get_random_start_position(racetrack)
        print_environment(racetrack,car_position = [old_y,old_x])
        best_starting_policy = policy[(old_y,old_x,0,0)] 
        if prob_y<= 0.2 and prob_x <= 0.2: 
          new_y, new_x, new_vy, new_vx = act(old_y, old_x, 0, 0, [0,0], racetrack)
        elif prob_y > 0.2 and prob_x <= 0.2: 
          new_y, new_x, new_vy, new_vx = act(old_y, old_x, 0, 0, [best_starting_policy[0],0], racetrack)
        elif prob_y <= 0.2 and prob_x > 0.2: 
          new_y, new_x, new_vy, new_vx= act(old_y, old_x, 0, 0, [0,best_starting_policy[1]], racetrack)
        elif prob_y > 0.2 and prob_x > 0.2: 
          new_y, new_x, new_vy, new_vx= act(old_y, old_x, 0, 0, [best_starting_policy[0],best_starting_policy[1]], racetrack)
        print_environment(racetrack, car_position = [new_y,new_x])
        i=1
        # And then keep movig the car until it reaches the finish line depending on the probabilities we get each time
        while True:
          old_y, old_x, old_vy, old_vx = new_y, new_x, new_vy, new_vx
          best_policy = policy[(old_y,old_x, old_vy,old_vx)] 
          prob_y = random()
          prob_x = random()
          if prob_y<= 0.2 and prob_x <= 0.2: # (accel on both axis fails)
            new_y, new_x, new_vy, new_vx = act(old_y, old_x, old_vy, old_vx, [0,0], racetrack)
          elif prob_y > 0.2 and prob_x <= 0.2: #(accel on x_axis fails, on y succeeds)
            new_y, new_x, new_vy, new_vx = act(old_y, old_x, old_vy, old_vx, [best_starting_policy[0],0], racetrack)
          elif prob_y <= 0.2 and prob_x > 0.2: #(accel on y_axis fails, on x succeeds)
            new_y, new_x, new_vy, new_vx= act(old_y, old_x, old_vy, old_vx, [0,best_starting_policy[1]], racetrack)
          elif prob_y > 0.2 and prob_x > 0.2: # (accel on both axis succeeds)
            new_y, new_x, new_vy, new_vx= act(old_y, old_x, old_vy, old_vx, [best_policy[0],best_policy[1]], racetrack)
          print_environment(racetrack, car_position = [new_y,new_x])
          i+=1
          if racetrack[new_y][new_x] == GOAL:
            print('''
            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣠⣤⣔⠒⠀⠉⠉⠢⡀⠀⠀⠀⠀⠀⠀⠀
            ⠀⠀⠀⠀⠀⠀⣀⣀⠠⠄⠒⠘⢿⣿⣿⣿⣿⣆⠀⠀⠀⠀⠱⡀⠀⠀⠀⠀⠀⠀
            ⢺⣦⢻⣿⣿⣿⣿⣄⠀⠀⠀⠀⠈⢿⡿⠿⠛⠛⠐⣶⣿⣿⣿⣧⡀⠀⠀⠀⠀⠀
            ⠈⢿⣧⢻⣿⣿⣿⣿⣆⣀⣠⣴⣶⣿⡄⠀⠀⠀⠀⠘⣿⣿⣿⣿⣧⠀⠀⠀⠀⠀
            ⠀⠀⢿⣧⢋⠉⠀⠀⠀⠹⣿⣿⣿⣿⣿⡆⣀⣤⣤⣶⣮⠀⠀⠀⠀⠣⠀⠀⠀⠀
            ⠀⠀⠈⢿⣧⢂⠀⠀⠀⠀⢘⠟⠛⠉⠁⠀⠹⣿⣿⣿⣿⣷⡀⠀⠀⠀⢣⠀⠀⠀
            ⠀⠀⠀⠈⢿⣧⢲⣶⣾⣿⣿⣧⡀⠀⠀⠀⢀⣹⠛⠋⠉⠉⠉⢿⣿⣿⣿⣧⠀⠀        
            ⠀⠀⠀⠀⠀⢿⣧⢻⣿⣿⣿⡿⠷⢤⣶⣿⣿⣿⣧⡀⠀⠀⠀⠈⢻⣿⣿⣿⣧⠀        In race #''' + str(race+1), ''' 
            ⠀⠀⠀⠀⠀⠈⢿⣧⢛⠉⠁⠀⠀⠀⢻⣿⣿⣿⡿⠗⠒⠒⠈⠉⠉⠉⠙⡉⠛⡃        Car finished in ''' + str(i) + ''' steps
            ⠀⠀⠀⠀⠀⠀⠈⢿⣯⢂⠀⠀⠀⡀⠤⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⠀⠀⠀⠀⠀⠀⠈⢿⣯⠐⠈⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢿⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
                            
                                
            ''')
            time.sleep(2)
            break

print_5_races(racetrack)